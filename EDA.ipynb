{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA) for the Spider Dataset\n",
    "\n",
    "This notebook explores the `train_spider.json` part of the Spider dataset. The goal is to understand the characteristics of the data, which includes:\n",
    "\n",
    "1.  **Dataset Overview**: Basic statistics and data format.\n",
    "2.  **Database Distribution**: How many unique databases are there, and how are the questions distributed among them?\n",
    "3.  **SQL Query Analysis**: What is the complexity of the SQL queries (length, keywords used)?\n",
    "4.  **Natural Language Analysis**: How long and complex are the user questions?\n",
    "\n",
    "These insights are crucial for building and evaluating our Text-to-SQL model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "# Set plot styles for better aesthetics\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data from the relative data directory\n",
    "try:\n",
    "    df = pd.read_json('../data/train_spider.json')\n",
    "    print(f\"Successfully loaded the dataset. Number of samples: {len(df)}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'train_spider.json' not found. Make sure it's in the 'data/' directory.\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Database Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"üß† Number of unique databases: {df['db_id'].nunique()}\")\n",
    "\n",
    "# Get the top 10 most common databases\n",
    "top_10_dbs = df['db_id'].value_counts().head(10)\n",
    "\n",
    "print(\"\\nüìä Top 10 most common databases:\")\n",
    "print(top_10_dbs)\n",
    "\n",
    "# Plot the distribution of the top 10 databases\n",
    "plt.figure(figsize=(12, 7))\n",
    "sns.barplot(x=top_10_dbs.values, y=top_10_dbs.index, palette='viridis')\n",
    "plt.title('Top 10 Most Common Databases in the Dataset')\n",
    "plt.xlabel('Number of Questions')\n",
    "plt.ylabel('Database ID')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insight**: The dataset contains questions across 140 unique databases. However, the distribution is not uniform, with databases like `college_2` and `college_1` appearing much more frequently. This might introduce a bias in the model if not handled carefully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. SQL Query Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. SQL Query Length (in characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sql_char_length'] = df['query'].str.len()\n",
    "\n",
    "print(\"üßÆ SQL Query Length Statistics (characters):\")\n",
    "print(df['sql_char_length'].describe())\n",
    "\n",
    "# Plot the distribution of SQL query lengths\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(df['sql_char_length'], bins=50, kde=True, color='blue')\n",
    "plt.title('Distribution of SQL Query Lengths (characters)')\n",
    "plt.xlabel('SQL Query Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insight**: Most SQL queries are between 50 and 150 characters long. The distribution is right-skewed, indicating the presence of a few very long and complex queries. This range is manageable for transformer models like BART."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. SQL Keyword Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = ['SELECT', 'WHERE', 'GROUP BY', 'ORDER BY', 'JOIN', 'HAVING', 'LIMIT', 'INTERSECT', 'EXCEPT', 'UNION']\n",
    "\n",
    "# Count how many times each keyword appears\n",
    "keyword_counts = Counter()\n",
    "for query in df['query']:\n",
    "    query_upper = query.upper()\n",
    "    for kw in keywords:\n",
    "        if kw in query_upper:\n",
    "            keyword_counts[kw] += 1\n",
    "\n",
    "# Create a DataFrame for plotting\n",
    "keyword_df = pd.DataFrame(keyword_counts.items(), columns=['Keyword', 'Frequency']).sort_values('Frequency', ascending=False)\n",
    "\n",
    "print(\"\\nüîç Frequency of SQL keywords in queries:\")\n",
    "print(keyword_df)\n",
    "\n",
    "# Plot the keyword frequency\n",
    "plt.figure(figsize=(12, 7))\n",
    "sns.barplot(data=keyword_df, x='Frequency', y='Keyword', palette='magma')\n",
    "plt.title('Frequency of Common SQL Keywords')\n",
    "plt.xlabel('Frequency Count')\n",
    "plt.ylabel('SQL Keyword')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insight**: As expected, `SELECT` is present in every query. `WHERE` and `JOIN` are very common, indicating that many questions require filtering and table relationships. `GROUP BY` and `ORDER BY` are also frequent. More complex clauses like `HAVING`, `INTERSECT`, and `EXCEPT` are much rarer, suggesting the model will have fewer examples to learn from for these cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Natural Language Question Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['nl_word_length'] = df['question'].apply(lambda x: len(x.split()))\n",
    "\n",
    "print(\"üßæ Natural Language Question Length Statistics (words):\")\n",
    "print(df['nl_word_length'].describe())\n",
    "\n",
    "# Plot the distribution of NL question lengths\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(df['nl_word_length'], bins=30, kde=True, color='green')\n",
    "plt.title('Distribution of NL Question Lengths (words)')\n",
    "plt.xlabel('Question Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insight**: The natural language questions are relatively short, with an average length of about 13 words. The distribution is fairly tight, with most questions falling between 5 and 20 words. This suggests that the input to our model will be concise and focused."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA Summary\n",
    "\n",
    "- The dataset is diverse, with 140 databases, but the question distribution is skewed towards a few popular ones.\n",
    "- SQL queries vary in length, with most being moderately complex. The model must handle common clauses like `JOIN`, `WHERE`, `GROUP BY`, and `ORDER BY` effectively.\n",
    "- Natural language inputs are short and direct, which simplifies the task of understanding user intent."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}